#!/bin/sh
set -eu

die() {
    echo "$1" >&2
    exit 1
}

tmpdir=
cleanup() {
    # if [ "$tmpdir" ]; then
    #     rm -rf "$tmpdir"
    # fi
    echo 'hi'
}
trap cleanup exit

main() {
    local function_name=gcc
    local local_headers=1
    local extra_packages=
    local gcc_package
    local dry_run=
    local yes=

    local temp
    temp=$(getopt -o hyn --long help,yes,dry-run,local-headers::,no-local-headers,extra-packages:: -n "$0" -- "$@")
    if [ $? -ne 0 ]; then
        exit 1
    fi
    eval set -- "$temp"
    unset temp
    while :; do
        case "$1" in
            --local-headers)
                case "$2" in
                    ''|'yes')
                        local_headers=1
                        ;;
                    'no')
                        local_headers=
                        ;;
                    *)
                        die "Bad value for --local-headers: $2"
                        ;;
                esac
                shift 2
                ;;
            --no-local-headers)
                local_headers=
                shift
                ;;
            --extra-packages)
                extra_packages="$2"
                shift 2
                ;;
            -n|--dry-run)
                dry_run=1
                shift
                ;;
            -y|--yes)
                yes=1
                shift
                ;;
            -h|--help)
                cat <<EOF >&2
Usage: $0 OPTIONS [FUNCTION-NAME]
  Build a GCC image and Lambda function suitable for llamacc.

  -h, --help
    Print this help

  -y, --yes
    Don't prompt for confirmation

  -n, --dry-run
    Don't upload the image or talk to AWS

  --local-headers[=yes|no], --no-local-headers
    Do/don't package the local system's C headers into the Llama
    container image. This provides an easy way to build anything that
    will build locally, but is very unreproducible.

  --extra-packages='package names'
    Install additional Debian packages into the image. This can be
    used to make additional development packages available for the
    remote GCC.
EOF
                exit 0
                ;;
            '--')
                shift
                break
                ;;
            *)
                die "Internal error: $1"
        esac
    done

    if [ "${1-}" ]; then
        function_name=$1
        shift
    fi

    if ! test -f /etc/os-release; then
        die "No /etc/os-release found: Can't detect OS version."
    fi
    . /etc/os-release
    local base_image
    case "$ID" in
        ubuntu)
        ;;
        debian)
        ;;
        pop)
        ;;
        *)
            die "Unsupported OS: $ID"
    esac
    base_image="$ID:$VERSION_CODENAME"
    local nvcc="$(which nvcc 2>/dev/null || :)"
    if [ -z "$nvcc" ]; then
        die "nvcc is not installed locally; I can't guess which version to use."
    fi
    local nvcc_package="$(dpkg-query -S "$(readlink -f "$nvcc")" | cut -f1 -d:)"
    if [ -z "$nvcc_package" ]; then
        die "$nvcc is not installed from apt; I can't guess which Debian package to use."
    fi
    local nvxx_package=$(echo "$nvcc_package" | sed s,gcc,g++,)
    tmpdir=$(mktemp -d --tmpdir llamacc.XXXXXXXX)

    mkdir -p "$tmpdir/build"
    cat > "$tmpdir/build/Dockerfile" <<EOF
FROM ghcr.io/nelhage/llama as llama
FROM ubuntu:22.04
# RUN apt-get update && apt-get -y install gcc g++ $nvcc_package $nvxx_package ca-certificates $extra_packages && apt-get clean

ENV CUDA_MAJOR_VERSION=11
ENV CUDA_MINOR_VERSION=7
ENV CUDA_PACKAGE_VERSION=11-7
ENV CUDA_VERSION=11.7
ENV NVIDIA_REQUIRE_CUDA="cuda>=11.7 driver>=470"

# install cuda repository
RUN apt-get update \
    && apt-get install -y curl unzip gnupg2 \
    && curl -sL https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin --output /etc/apt/preferences.d/cuda-repository-pin-600 \
    && apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/3bf863cc.pub \
    && echo "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /" > /etc/apt/sources.list.d/cuda.list \
    && rm -rf /var/lib/apt/lists/*

RUN apt-get update \
    && apt-get install -y zlib1g cuda-compat-11-7 libcusolver-11-7 libcusparse-11-7 libcublas-11-7
    # && rm -rf /var/lib/apt/lists/*
ENV LD_LIBRARY_PATH /usr/local/cuda/compat:/usr/local/cuda/lib64

COPY --from=llama /llama_runtime /llama_runtime
WORKDIR /
ENTRYPOINT ["/llama_runtime"]
EOF
    if [ "$local_headers" ]; then
        echo "Gathering locally-installed header files..."
        local search_paths
        nvcc -E -v /dev/null -o /dev/null 2>&1 | grep -Ee 'INCLUDES'
        echo 'after manual run'
        search_paths=$(nvcc -E -v /dev/null -o /dev/null 2>&1 | grep -Ee 'INCLUDES')
        find $search_paths -type f -print | sort -u | tar -Pczf "$tmpdir/build/headers.tgz" --files-from=/dev/stdin
        echo 'after search paths'
        cat >> "$tmpdir/build/Dockerfile" <<EOF
ADD headers.tgz /
EOF
    fi

    echo "I am going to build a Docker image and a Lambda function. Details:"
    echo "  Function name: $function_name"
    echo "  Base image: $base_image"
    echo "  nvCC package: $nvcc_package"
    if [ "$extra_packages" ]; then
        echo "  Extra packages: $extra_packages"
    fi
    if [ "$local_headers" ]; then
        echo "  Package local header files: yes"
    else
        echo "  Package local header files: no"
    fi

    if [ "$dry_run" ]; then
        echo "Executing dry-run Docker build:"
        echo
        docker build "$tmpdir/build"
        exit
    else
        if [ ! "$yes" ]; then
            while :; do
                echo -n "Continue [Y/n]? "
                local reply
                read -r reply
                case $reply in
                    y|Y|'')
                        break
                        ;;
                    n|N)
                        echo "Exiting..."
                        exit 1
                        ;;
                    *)
                        ;;
                esac
            done
        fi
    fi

    llama update-function --create --build="$tmpdir/build" "$function_name"
}

main "$@"
